{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sales Forecasting System - Data Cleaning\n",
    "\n",
    "This notebook implements comprehensive data cleaning for the sales forecasting dataset.\n",
    "\n",
    "## Steps:\n",
    "1. Load raw data\n",
    "2. Handle missing values\n",
    "3. Remove duplicates\n",
    "4. Fix data types\n",
    "5. Handle outliers\n",
    "6. Validate and save cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load raw sales data\n",
    "# df = pd.read_csv('raw_sales_data.csv')\n",
    "\n",
    "# For demonstration, create sample data\n",
    "np.random.seed(42)\n",
    "dates = pd.date_range('2020-01-01', '2023-12-31', freq='D')\n",
    "n_samples = len(dates)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'sales': np.random.normal(5000, 1500, n_samples) + np.arange(n_samples) * 2,\n",
    "    'quantity': np.random.randint(50, 500, n_samples),\n",
    "    'price': np.random.uniform(10, 100, n_samples),\n",
    "    'region': np.random.choice(['North', 'South', 'East', 'West'], n_samples),\n",
    "    'product_category': np.random.choice(['Electronics', 'Clothing', 'Food', 'Books'], n_samples)\n",
    "})\n",
    "\n",
    "# Introduce some missing values\n",
    "df.loc[np.random.choice(df.index, 50), 'sales'] = np.nan\n",
    "df.loc[np.random.choice(df.index, 30), 'quantity'] = np.nan\n",
    "\n",
    "# Introduce duplicates\n",
    "df = pd.concat([df, df.iloc[:20]], ignore_index=True)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Data quality assessment\n",
    "print(\"=== Data Quality Report ===")\n",
    "print(f\"\\nDataset Shape: {df.shape}\")\n",
    "print(f\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"\\nData Types:\")\n",
    "print(df.dtypes)\n",
    "print(f\"\\nDuplicate Rows: {df.duplicated().sum()}")
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Remove duplicate rows\n",
    "print(f\"Rows before removing duplicates: {len(df)}\")\n",
    "df = df.drop_duplicates()\n",
    "print(f\"Rows after removing duplicates: {len(df)}\")\n",
    "df.reset_index(drop=True, inplace=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Handle missing values\n",
    "print(\"\\nHandling missing values...\")\n",
    "\n",
    "# For sales: fill with median grouped by product_category\n",
    "df['sales'] = df.groupby('product_category')['sales'].transform(\n",
    "    lambda x: x.fillna(x.median())\n",
    ")\n",
    "\n",
    "# For quantity: fill with median\n",
    "df['quantity'] = df['quantity'].fillna(df['quantity'].median())\n",
    "\n",
    "print(\"Missing values after imputation:\")\n",
    "print(df.isnull().sum())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Fix data types\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['quantity'] = df['quantity'].astype(int)\n",
    "df['sales'] = df['sales'].astype(float)\n",
    "df['price'] = df['price'].astype(float)\n",
    "\n",
    "print(\"Data types after correction:\")\n",
    "print(df.dtypes)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Detect and handle outliers using IQR method\n",
    "def detect_outliers_iqr(data, column):\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return (data[column] < lower_bound) | (data[column] > upper_bound)\n",
    "\n",
    "# Identify outliers\n",
    "outliers_sales = detect_outliers_iqr(df, 'sales')\n",
    "print(f\"Number of outliers in sales: {outliers_sales.sum()}\")\n",
    "\n",
    "# Cap outliers instead of removing\n",
    "Q1_sales = df['sales'].quantile(0.25)\n",
    "Q3_sales = df['sales'].quantile(0.75)\n",
    "IQR_sales = Q3_sales - Q1_sales\n",
    "df['sales'] = df['sales'].clip(\n",
    "    lower=Q1_sales - 1.5 * IQR_sales,\n",
    "    upper=Q3_sales + 1.5 * IQR_sales\n",
    ")\n",
    "\n",
    "print(\"Outliers handled successfully")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize cleaned data distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Sales distribution\n",
    "axes[0, 0].hist(df['sales'], bins=50, edgecolor='black')\n",
    "axes[0, 0].set_title('Sales Distribution (After Cleaning)')\n",
    "axes[0, 0].set_xlabel('Sales')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Sales over time\n",
    "axes[0, 1].plot(df['date'], df['sales'], alpha=0.5)\n",
    "axes[0, 1].set_title('Sales Over Time')\n",
    "axes[0, 1].set_xlabel('Date')\n",
    "axes[0, 1].set_ylabel('Sales')\n",
    "\n",
    "# Sales by region\n",
    "df.groupby('region')['sales'].mean().plot(kind='bar', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Average Sales by Region')\n",
    "axes[1, 0].set_xlabel('Region')\n",
    "axes[1, 0].set_ylabel('Average Sales')\n",
    "\n",
    "# Sales by product category\n",
    "df.groupby('product_category')['sales'].mean().plot(kind='bar', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Average Sales by Product Category')\n",
    "axes[1, 1].set_xlabel('Product Category')\n",
    "axes[1, 1].set_ylabel('Average Sales')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Final data validation\n",
    "print(\"=== Final Data Quality Report ===")\n",
    "print(f\"\\nFinal Dataset Shape: {df.shape}\")\n",
    "print(f\"\\nMissing Values: {df.isnull().sum().sum()}\")\n",
    "print(f\"\\nDuplicate Rows: {df.duplicated().sum()}\")\n",
    "print(f\"\\nDate Range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"\\nSummary Statistics:\")\n",
    "print(df.describe())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save cleaned data\n",
    "# df.to_csv('cleaned_sales_data.csv', index=False)\n",
    "print(\"\\nData cleaning completed successfully!\")\n",
    "print(f\"Cleaned dataset saved with {len(df)} rows and {len(df.columns)} columns")"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
